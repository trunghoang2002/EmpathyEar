{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/.conda/envs/empathyear/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 1.32k/1.32k [00:00<00:00, 125kB/s]\n",
      "Downloading (…)iguration_chatglm.py: 100%|██████████| 2.33k/2.33k [00:00<00:00, 244kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading modeling_chatglm.py: 100%|██████████| 56.5k/56.5k [00:00<00:00, 5.55MB/s]\n",
      "Downloading quantization.py: 100%|██████████| 14.7k/14.7k [00:00<00:00, 5.11MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 21.2k/21.2k [00:00<00:00, 3.22MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.83G/1.83G [00:55<00:00, 32.7MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.97G/1.97G [01:00<00:00, 32.6MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.93G/1.93G [01:20<00:00, 24.1MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.82G/1.82G [00:55<00:00, 32.5MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.97G/1.97G [01:00<00:00, 32.6MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.93G/1.93G [00:59<00:00, 32.7MB/s]\n",
      "Downloading (…)of-00007.safetensors: 100%|██████████| 1.05G/1.05G [00:32<00:00, 32.7MB/s]\n",
      "Downloading shards: 100%|██████████| 7/7 [06:47<00:00, 58.21s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:04<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Tải mô hình về local\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "\n",
    "# Lưu mô hình vào thư mục local\n",
    "model.save_pretrained(\"/home/hoang/EmpathyEar/chatglm/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1minfo:\u001b[0m downloading installer\n",
      "\u001b[0m\u001b[1m\n",
      "Welcome to Rust!\n",
      "\u001b[0m\n",
      "This will download and install the official compiler for the Rust\n",
      "programming language, and its package manager, Cargo.\n",
      "\n",
      "Rustup metadata and toolchains will be installed into the Rustup\n",
      "home directory, located at:\n",
      "\n",
      "  /home/hoang/.rustup\n",
      "\n",
      "This can be modified with the RUSTUP_HOME environment variable.\n",
      "\n",
      "The Cargo home directory is located at:\n",
      "\n",
      "  /home/hoang/.cargo\n",
      "\n",
      "This can be modified with the CARGO_HOME environment variable.\n",
      "\n",
      "The \u001b[0m\u001b[1mcargo\u001b[0m, \u001b[0m\u001b[1mrustc\u001b[0m, \u001b[0m\u001b[1mrustup\u001b[0m and other commands will be added to\n",
      "Cargo's bin directory, located at:\n",
      "\n",
      "  /home/hoang/.cargo/bin\n",
      "\n",
      "This path will then be added to your \u001b[0m\u001b[1mPATH\u001b[0m environment variable by\n",
      "modifying the profile files located at:\n",
      "\n",
      "  /home/hoang/.profile\n",
      "  /home/hoang/.bash_profile\n",
      "  /home/hoang/.bashrc\n",
      "\n",
      "You can uninstall at any time with \u001b[0m\u001b[1mrustup self uninstall\u001b[0m and\n",
      "these changes will be reverted.\n",
      "\n",
      "Current installation options:\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1m   \u001b[0mdefault host triple: \u001b[0m\u001b[1mx86_64-unknown-linux-gnu\u001b[0m\n",
      "\u001b[0m\u001b[1m     \u001b[0mdefault toolchain: \u001b[0m\u001b[1mstable (default)\u001b[0m\n",
      "\u001b[0m\u001b[1m               \u001b[0mprofile: \u001b[0m\u001b[1mdefault\u001b[0m\n",
      "  modify PATH variable: \u001b[0m\u001b[1myes\u001b[0m\n",
      "\n",
      "1) Proceed with standard installation (default - just press enter)\n",
      "2) Customize installation\n",
      "3) Cancel installation\n",
      ">^C\n"
     ]
    }
   ],
   "source": [
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs -o rustup.sh\n",
    "!sh rustup.sh -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source \"$HOME/.cargo/env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoang/EmpathyEar/chatglm\n",
      "Collecting protobuf>=4.25.3 (from -r requirements.txt (line 3))\n",
      "  Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting transformers>=4.39.3 (from -r requirements.txt (line 4))\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tokenizers>=0.15.0 (from -r requirements.txt (line 5))\n",
      "  Using cached tokenizers-0.21.0.tar.gz (343 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cpm_kernels>=1.0.11 (from -r requirements.txt (line 6))\n",
      "  Downloading cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting torch>=2.1.0 (from -r requirements.txt (line 7))\n",
      "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting gradio>=4.26.0 (from -r requirements.txt (line 8))\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from -r requirements.txt (line 9))\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting sentence_transformers>=2.4.0 (from -r requirements.txt (line 10))\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting accelerate>=0.29.2 (from -r requirements.txt (line 11))\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting streamlit>=1.33.0 (from -r requirements.txt (line 12))\n",
      "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting fastapi>=0.110.0 (from -r requirements.txt (line 13))\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting loguru~=0.7.2 (from -r requirements.txt (line 14))\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting mdtex2html==1.3.0 (from -r requirements.txt (line 15))\n",
      "  Downloading mdtex2html-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting latex2mathml==3.75.0 (from -r requirements.txt (line 16))\n",
      "  Downloading latex2mathml-3.75.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jupyter_client>=8.6.1 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (8.6.3)\n",
      "Collecting markdown (from mdtex2html==1.3.0->-r requirements.txt (line 15))\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting filelock (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from transformers>=4.39.3->-r requirements.txt (line 4)) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers>=0.15.0 (from -r requirements.txt (line 5))\n",
      "  Downloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from torch>=2.1.0->-r requirements.txt (line 7)) (4.12.2)\n",
      "Collecting sympy (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jinja2 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading anyio-4.5.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting ffmpy (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting markupsafe~=2.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting matplotlib~=3.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting orjson~=3.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading orjson-3.10.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic>=2.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydub (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting urllib3~=2.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading uvicorn-0.33.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading websockets-12.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting scikit-learn (from sentence_transformers>=2.4.0->-r requirements.txt (line 10))\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers>=2.4.0->-r requirements.txt (line 10))\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Requirement already satisfied: psutil in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from accelerate>=0.29.2->-r requirements.txt (line 11)) (6.0.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyarrow>=7.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from streamlit>=1.33.0->-r requirements.txt (line 12)) (6.4.1)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi>=0.110.0->-r requirements.txt (line 13))\n",
      "  Downloading starlette-0.44.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from jupyter_client>=8.6.1->-r requirements.txt (line 17)) (8.5.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from jupyter_client>=8.6.1->-r requirements.txt (line 17)) (5.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from jupyter_client>=8.6.1->-r requirements.txt (line 17)) (2.9.0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from jupyter_client>=8.6.1->-r requirements.txt (line 17)) (25.1.2)\n",
      "Requirement already satisfied: traitlets>=5.3 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from jupyter_client>=8.6.1->-r requirements.txt (line 17)) (5.14.3)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading narwhals-1.25.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting idna>=2.8 (from anyio<5.0,>=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5.0,>=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting certifi (from httpx>=0.24.1->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter_client>=8.6.1->-r requirements.txt (line 17)) (3.21.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter_client>=8.6.1->-r requirements.txt (line 17)) (4.3.6)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading fonttools-4.55.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib~=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas<3.0,>=1.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter_client>=8.6.1->-r requirements.txt (line 17)) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers>=4.39.3->-r requirements.txt (line 4))\n",
      "  Downloading charset_normalizer-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hoang/.conda/envs/my_env/lib/python3.8/site-packages (from rich<14,>=10.14.0->streamlit>=1.33.0->-r requirements.txt (line 12)) (2.18.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn->sentence_transformers>=2.4.0->-r requirements.txt (line 10))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers>=2.4.0->-r requirements.txt (line 10))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=2.1.0->-r requirements.txt (line 7))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl.metadata (624 bytes)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading rpds_py-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.33.0->-r requirements.txt (line 12))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading mdtex2html-1.3.0-py3-none-any.whl (13 kB)\n",
      "Downloading latex2mathml-3.75.0-py3-none-any.whl (72 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Downloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
      "Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.5.2-py3-none-any.whl (89 kB)\n",
      "Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.5/746.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.44.0-py3-none-any.whl (73 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading uvicorn-0.33.0-py3-none-any.whl (62 kB)\n",
      "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading fonttools-4.55.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading narwhals-1.25.0-py3-none-any.whl (313 kB)\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading websockets-12.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: sentencepiece, pytz, pydub, mpmath, cpm_kernels, websockets, watchdog, urllib3, tzdata, tqdm, tomlkit, toml, threadpoolctl, tenacity, sympy, sniffio, smmap, shellingham, semantic-version, safetensors, ruff, rpds-py, regex, pyyaml, python-multipart, pyparsing, pydantic-core, protobuf, pkgutil-resolve-name, pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, mdurl, markupsafe, loguru, latex2mathml, kiwisolver, joblib, importlib-resources, idna, h11, fsspec, fonttools, filelock, ffmpy, exceptiongroup, cycler, click, charset-normalizer, certifi, cachetools, blinker, attrs, annotated-types, aiofiles, uvicorn, triton, scipy, requests, referencing, pydantic, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, markdown, jinja2, httpcore, gitdb, contourpy, anyio, starlette, scikit-learn, rich, pydeck, nvidia-cusolver-cu12, mdtex2html, matplotlib, jsonschema-specifications, huggingface-hub, httpx, gitpython, typer, torch, tokenizers, jsonschema, gradio-client, fastapi, transformers, gradio, altair, accelerate, streamlit, sentence_transformers\n",
      "Successfully installed accelerate-1.0.1 aiofiles-23.2.1 altair-5.4.1 annotated-types-0.7.0 anyio-4.5.2 attrs-25.1.0 blinker-1.8.2 cachetools-5.5.1 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 contourpy-1.1.1 cpm_kernels-1.0.11 cycler-0.12.1 exceptiongroup-1.2.2 fastapi-0.115.8 ffmpy-0.5.0 filelock-3.16.1 fonttools-4.55.8 fsspec-2025.2.0 gitdb-4.0.12 gitpython-3.1.44 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.28.1 idna-3.10 importlib-resources-6.4.5 jinja2-3.1.5 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.7 latex2mathml-3.75.0 loguru-0.7.3 markdown-3.7 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.7.5 mdtex2html-1.3.0 mdurl-0.1.2 mpmath-1.3.0 narwhals-1.25.0 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.8.61 nvidia-nvtx-cu12-12.1.105 orjson-3.10.15 pandas-2.0.3 pillow-10.4.0 pkgutil-resolve-name-1.3.10 protobuf-5.29.3 pyarrow-17.0.0 pydantic-2.10.6 pydantic-core-2.27.2 pydeck-0.9.1 pydub-0.25.1 pyparsing-3.1.4 python-multipart-0.0.20 pytz-2025.1 pyyaml-6.0.2 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 rpds-py-0.20.1 ruff-0.9.4 safetensors-0.5.2 scikit-learn-1.3.2 scipy-1.10.1 semantic-version-2.10.0 sentence_transformers-3.2.1 sentencepiece-0.2.0 shellingham-1.5.4 smmap-5.0.2 sniffio-1.3.1 starlette-0.44.0 streamlit-1.40.1 sympy-1.13.3 tenacity-9.0.0 threadpoolctl-3.5.0 tokenizers-0.20.3 toml-0.10.2 tomlkit-0.12.0 torch-2.4.1 tqdm-4.67.1 transformers-4.46.3 triton-3.0.0 typer-0.15.1 tzdata-2025.1 urllib3-2.2.3 uvicorn-0.33.0 watchdog-4.0.2 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hoang/EmpathyEar/chatglm\n",
    "!export PATH=\"$HOME/.cargo/bin:$PATH\" && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "++ LR=1e-4\n",
      "++ NUM_GPUS=1\n",
      "++ LORA_RANK=8\n",
      "++ LORA_ALPHA=32\n",
      "++ LORA_DROUPOUT=0.1\n",
      "++ MAX_SOURCE_LEN=2048\n",
      "++ MAX_TARGET_LEN=1024\n",
      "++ DEV_BATCH_SIZE=1\n",
      "++ GRAD_ACCUMULARION_STEPS=1\n",
      "++ NUM_TRAIN_EPOCHS=4\n",
      "++ SAVE_INTERVAL=24000\n",
      "++ MAX_SEQ_LEN=2048\n",
      "++ RUN_NAME=EED_4epoch\n",
      "++ BASE_MODEL_PATH=THUDM/chatglm3-6b\n",
      "++ DATASET_PATH=../data/EED/EED4LLM_6000.json\n",
      "+++ date +%Y%m%d-%H%M%S\n",
      "++ DATESTR=20250204-024948\n",
      "++ OUTPUT_DIR=output/EED_4epoch-20250204-024948-1e-4\n",
      "+++ shuf -n 1 -i 10000-65535\n",
      "++ MASTER_PORT=48899\n",
      "++ mkdir -p output/EED_4epoch-20250204-024948-1e-4\n",
      "++ python finetune.py --train_format input-output --train_file ../data/EED/EED4LLM_6000.json --lora_rank 8 --lora_alpha 32 --lora_dropout 0.1 --max_seq_length 2048 --preprocessing_num_workers 1 --model_name_or_path THUDM/chatglm3-6b --output_dir output/EED_4epoch-20250204-024948-1e-4 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --num_train_epochs 4 --logging_steps 1 --save_steps 24000 --learning_rate 1e-4\n",
      "++ tee output/EED_4epoch-20250204-024948-1e-4/train.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04/2025 02:49:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2distributed training: True, 16-bits training: False\n",
      "02/04/2025 02:49:54 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/EED_4epoch-20250204-024948-1e-4/runs/Feb04_02-49-53_vjtechnologies,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=output/EED_4epoch-20250204-024948-1e-4,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/EED_4epoch-20250204-024948-1e-4,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=24000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-04 02:49:54,738 >> loading file tokenizer.model from cache at /home/hoang/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/e9e0406d062cdb887444fe5bd546833920abd4ac/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-04 02:49:54,738 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-04 02:49:54,738 >> loading file special_tokens_map.json from cache at /home/hoang/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/e9e0406d062cdb887444fe5bd546833920abd4ac/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-04 02:49:54,738 >> loading file tokenizer_config.json from cache at /home/hoang/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/e9e0406d062cdb887444fe5bd546833920abd4ac/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-02-04 02:49:54,738 >> loading file tokenizer.json from cache at None\n",
      "02/04/2025 02:49:54 - WARNING - transformers_modules.THUDM.chatglm3-6b.e9e0406d062cdb887444fe5bd546833920abd4ac.tokenization_chatglm - Setting eos_token is not supported, use the default one.\n",
      "02/04/2025 02:49:54 - WARNING - transformers_modules.THUDM.chatglm3-6b.e9e0406d062cdb887444fe5bd546833920abd4ac.tokenization_chatglm - Setting pad_token is not supported, use the default one.\n",
      "02/04/2025 02:49:54 - WARNING - transformers_modules.THUDM.chatglm3-6b.e9e0406d062cdb887444fe5bd546833920abd4ac.tokenization_chatglm - Setting unk_token is not supported, use the default one.\n",
      "[INFO|configuration_utils.py:679] 2025-02-04 02:49:55,693 >> loading configuration file config.json from cache at /home/hoang/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/e9e0406d062cdb887444fe5bd546833920abd4ac/config.json\n",
      "[INFO|configuration_utils.py:679] 2025-02-04 02:49:56,012 >> loading configuration file config.json from cache at /home/hoang/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/e9e0406d062cdb887444fe5bd546833920abd4ac/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-02-04 02:49:56,015 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3937] 2025-02-04 02:49:56,879 >> loading weights file model.safetensors from cache at /home/hoang/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/e9e0406d062cdb887444fe5bd546833920abd4ac/model.safetensors.index.json\n",
      "[INFO|configuration_utils.py:1096] 2025-02-04 02:49:56,884 >> Generate config GenerationConfig {\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.81it/s]\n",
      "[INFO|modeling_utils.py:4800] 2025-02-04 02:49:58,857 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2025-02-04 02:49:58,858 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm3-6b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:4272] 2025-02-04 02:49:59,194 >> Generation config file not found, using a generation config created from the model config.\n",
      "Traceback (most recent call last):\n",
      "  File \"finetune.py\", line 185, in <module>\n",
      "    main()\n",
      "  File \"finetune.py\", line 94, in main\n",
      "    model = AutoModel.from_pretrained(model_args.model_name_or_path, trust_remote_code=True).cuda()\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3117, in cuda\n",
      "    return super().cuda(*args, **kwargs)\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 916, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 3 more times]\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/hoang/.conda/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 916, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 116.00 MiB is free. Process 3497858 has 2.51 GiB memory in use. Process 3498806 has 14.80 GiB memory in use. Process 3509749 has 2.61 GiB memory in use. Including non-PyTorch memory, this process has 3.61 GiB memory in use. Of the allocated memory 3.23 GiB is allocated by PyTorch, and 1.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/hoang/EmpathyEar/chatglm\n",
    "CUDA_VISIBLE_DEVICES=1\n",
    "export NCCL_P2P_DISABLE=\"1\"\n",
    "export NCCL_IB_DISABLE=\"1\"\n",
    "source ./scripts/finetune_lora.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
